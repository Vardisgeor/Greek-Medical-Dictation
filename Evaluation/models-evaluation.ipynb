{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Whisper Model Evaluation for Greek ASR\n","\n","This notebook evaluates the performance of six Whisper models (base and fine-tuned versions of Small, Medium, and Large-v2) for automatic speech recognition (ASR) on Greek audio. The focus is to compare base and fine-tuned models on a combined Greek dataset using standard ASR metrics.\n","\n","\n","## Objective\n","\n","* Evaluate Model Performance: Assess base and fine-tuned Whisper models for Greek ASR accuracy.\n","* Metrics: Word Error Rate (WER), Normalized WER, and Character Error Rate (CER).\n","* Dataset: A combined test set from \"Vardis/Greek_Mosel\", Common Voice (Greek), and Fleurs (Greek), standardized to 16kHz audio.\n","\n","## Workflow\n","\n","### Dataset Preparation:\n","\n","* Load and split datasets: \"Vardis/Greek_Mosel\", Common Voice 11.0 (el), and Fleurs (el_gr).\n","* Combine and shuffle train, validation, and test splits (80% train, 10% validation, 10% test).\n","* Standardize audio to 16kHz and rename text fields to sentence.\n","\n","\n","### Model Setup:\n","\n","* Load base Whisper models: openai/whisper-small, openai/whisper-medium, openai/whisper-large-v2.\n","* Load fine-tuned models: Vardis/Whisper-Small-Greek, Vardis/Whisper-Medium-Greek, Vardis/Whisper-LoRA-Greek (with LoRA weights merged).\n","* Use torch.float16 and device_map=\"auto\" for GPU acceleration.\n","\n","\n","### Evaluation Process:\n","\n","* For each model (base and fine-tuned):\n","- Generate transcriptions for the test set using greedy decoding (max_length=225).\n","- Collect predictions and reference sentences.\n","- Compute WER, Normalized WER, and CER.\n","\n","\n","### Results:\n","\n","* Report metrics for each model:\n","- WER: Word-level errors (standard).\n","- Normalized WER: WER after lowercasing, removing punctuation, and standardizing whitespace.\n","- CER: Character-level errors.\n","* Compare base vs. fine-tuned performance to assess the impact of fine-tuning.\n","\n","\n","This evaluation highlights the improvements of fine-tuned Whisper models over their base counterparts for Greek ASR tasks.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-08-26T13:13:49.550012Z","iopub.status.busy":"2025-08-26T13:13:49.549766Z","iopub.status.idle":"2025-08-26T13:13:58.454458Z","shell.execute_reply":"2025-08-26T13:13:58.453559Z","shell.execute_reply.started":"2025-08-26T13:13:49.549990Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n","Collecting jiwer\n","  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.5.1)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n","Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\n","Collecting rapidfuzz>=3.9.7 (from jiwer)\n","  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n","Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n","  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n","Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: rapidfuzz, fsspec, jiwer, evaluate\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.5.1\n","    Uninstalling fsspec-2025.5.1:\n","      Successfully uninstalled fsspec-2025.5.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n","cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n","bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n","bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed evaluate-0.4.5 fsspec-2025.3.0 jiwer-4.0.0 rapidfuzz-3.13.0\n"]}],"source":["!pip install evaluate jiwer\n","!pip install datasets==3.6.0\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-08-26T13:13:58.455977Z","iopub.status.busy":"2025-08-26T13:13:58.455639Z","iopub.status.idle":"2025-08-26T13:18:18.771667Z","shell.execute_reply":"2025-08-26T13:18:18.770851Z","shell.execute_reply.started":"2025-08-26T13:13:58.455944Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e27265098e874cdeb4d868116455c972","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/322 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d477134145494069b689b0884008a838","version_major":2,"version_minor":0},"text/plain":["data/train-00000-of-00007.parquet:   0%|          | 0.00/497M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d89e1897679545e9ae503e643ec0663e","version_major":2,"version_minor":0},"text/plain":["data/train-00001-of-00007.parquet:   0%|          | 0.00/494M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23b3c27460c44cfba8d29df98ad298c2","version_major":2,"version_minor":0},"text/plain":["data/train-00002-of-00007.parquet:   0%|          | 0.00/498M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"043810b9ed3d4ab7a845bb0f03d1231c","version_major":2,"version_minor":0},"text/plain":["data/train-00003-of-00007.parquet:   0%|          | 0.00/497M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59ddd535972f4893991ef492e9442400","version_major":2,"version_minor":0},"text/plain":["data/train-00004-of-00007.parquet:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"263db1bf18e749f2a0ac867ca7f53e63","version_major":2,"version_minor":0},"text/plain":["data/train-00005-of-00007.parquet:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"887cc57144004140bd35177583064321","version_major":2,"version_minor":0},"text/plain":["data/train-00006-of-00007.parquet:   0%|          | 0.00/505M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a07e469fcc740fdbfe7ec1263034bc9","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/3876 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a776a77c1f54d9cafa65cf9289938b8","version_major":2,"version_minor":0},"text/plain":["README.md: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e0f01cc9e564db7a36cc0a694fb59a1","version_major":2,"version_minor":0},"text/plain":["common_voice_11_0.py: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dbb5831c76c8446888ad99025a8556e0","version_major":2,"version_minor":0},"text/plain":["languages.py: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0fac0f7f56c40a0b8568f81cf279707","version_major":2,"version_minor":0},"text/plain":["release_stats.py: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N]  y\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed6c2251282849779061f8384de37ff0","version_major":2,"version_minor":0},"text/plain":["n_shards.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"63e8304f2b614ef6aefc839f267d5fcc","version_major":2,"version_minor":0},"text/plain":["audio/el/train/el_train_0.tar:   0%|          | 0.00/57.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"039fe52e96504cf3854de527244c85ef","version_major":2,"version_minor":0},"text/plain":["audio/el/dev/el_dev_0.tar:   0%|          | 0.00/51.0M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9cae51c50ad14c28be07b7bebf26fbca","version_major":2,"version_minor":0},"text/plain":["audio/el/test/el_test_0.tar:   0%|          | 0.00/50.9M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e7e4b242410f4d2bb8c91a5bd4c7ed79","version_major":2,"version_minor":0},"text/plain":["audio/el/other/el_other_0.tar:   0%|          | 0.00/238M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de87843204f64ebabe5e164a9b46b3d7","version_major":2,"version_minor":0},"text/plain":["audio/el/invalidated/el_invalidated_0.ta(…):   0%|          | 0.00/23.3M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fafe723fde4d4c2382e83f1c8e1d2585","version_major":2,"version_minor":0},"text/plain":["transcript/el/train.tsv:   0%|          | 0.00/482k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68110d3510cd4977ad3f8f7b48f1fc11","version_major":2,"version_minor":0},"text/plain":["transcript/el/dev.tsv:   0%|          | 0.00/423k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"924cf69248ae40eb919c4993e77de6ae","version_major":2,"version_minor":0},"text/plain":["transcript/el/test.tsv:   0%|          | 0.00/410k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9011a85f81149258a8851df3ecda58c","version_major":2,"version_minor":0},"text/plain":["transcript/el/other.tsv:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60a34244fbd44593a7e049d8097dc992","version_major":2,"version_minor":0},"text/plain":["transcript/el/invalidated.tsv:   0%|          | 0.00/201k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aef39551aa544ace8447a97e0c225355","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","Reading metadata...: 1914it [00:00, 130308.21it/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"47422fb29c124a07bcee11c3fd43da89","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","Reading metadata...: 1701it [00:00, 138604.17it/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4ce94fbd43e4faf8482116dc6f6e822","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","Reading metadata...: 1696it [00:00, 132018.25it/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"544305adb726414c8eebab1575c8981c","version_major":2,"version_minor":0},"text/plain":["Generating other split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","Reading metadata...: 9072it [00:00, 142940.37it/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7bbe35c7cedf4f69afbc2951e49ff9c3","version_major":2,"version_minor":0},"text/plain":["Generating invalidated split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","Reading metadata...: 797it [00:00, 97687.33it/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ed92eb971e54ebaaf8ddd04bd260830","version_major":2,"version_minor":0},"text/plain":["README.md: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78eae2361a9044cfb75e528a5a30467f","version_major":2,"version_minor":0},"text/plain":["fleurs.py: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["The repository for google/fleurs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/google/fleurs.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N]  y\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2eabb98f73f841c09377d94b2a28768a","version_major":2,"version_minor":0},"text/plain":["data/el_gr/audio/train.tar.gz:   0%|          | 0.00/1.91G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2dcdba80f4aa478abe39de566c883b74","version_major":2,"version_minor":0},"text/plain":["data/el_gr/audio/dev.tar.gz:   0%|          | 0.00/141M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2d12d0203384855ae80f7ada00ac2e9","version_major":2,"version_minor":0},"text/plain":["data/el_gr/audio/test.tar.gz:   0%|          | 0.00/349M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9271a4cd6b29467c8b44c6beb38c1dcc","version_major":2,"version_minor":0},"text/plain":["train.tsv: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a51a2a029fcb4100bffd84869d026ac1","version_major":2,"version_minor":0},"text/plain":["dev.tsv: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cded6b6a35364f26843186abe0b55ea2","version_major":2,"version_minor":0},"text/plain":["test.tsv: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0cfca16e06c47b8bc3f3d2f5c2a1c18","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb4aa343ec194c278ee3a15e2afcbf34","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"840d2f4cd30c49658d8e78db031dd591","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["IterableDatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 3100\n","    })\n","    validation: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 388\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 388\n","    })\n","})\n","IterableDatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 4248\n","    })\n","    validation: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 531\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 532\n","    })\n","})\n","IterableDatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 3308\n","    })\n","    validation: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 414\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 414\n","    })\n","})\n","IterableDatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 10656\n","    })\n","    validation: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 1333\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 1334\n","    })\n","})\n"]}],"source":["from datasets import load_dataset, IterableDatasetDict\n","import os\n","\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","language = \"Greek\"\n","language_abbr = \"el\"\n","language_abbr2 = \"el_gr\"\n","task = \"transcribe\"\n","\n","\n","a = IterableDatasetDict()\n","b = IterableDatasetDict()\n","c = IterableDatasetDict()\n","\n","\n","a_full = load_dataset(\"Vardis/Greek_Mosel\", split=\"train\")\n","a_temp = a_full.train_test_split(test_size=0.2, seed=42)  # 80% train \n","a_val_test = a_temp[\"test\"].train_test_split(test_size=0.5, seed=42)  # 10% val + 10% test\n","a[\"train\"] = a_temp[\"train\"]\n","a[\"validation\"] = a_val_test[\"train\"]\n","a[\"test\"] = a_val_test[\"test\"]\n","\n","b_full = load_dataset(\"mozilla-foundation/common_voice_11_0\", language_abbr, split=\"train+validation+test\")\n","b_temp = b_full.train_test_split(test_size=0.2, seed=42)\n","b_val_test = b_temp[\"test\"].train_test_split(test_size=0.5, seed=42)\n","b[\"train\"] = b_temp[\"train\"]\n","b[\"validation\"] = b_val_test[\"train\"]\n","b[\"test\"] = b_val_test[\"test\"]\n","\n","c_full = load_dataset(\"google/fleurs\", language_abbr2, split=\"train+validation+test\")\n","c_temp = c_full.train_test_split(test_size=0.2, seed=42)\n","c_val_test = c_temp[\"test\"].train_test_split(test_size=0.5, seed=42)\n","c[\"train\"] = c_temp[\"train\"]\n","c[\"validation\"] = c_val_test[\"train\"]\n","c[\"test\"] = c_val_test[\"test\"]\n","\n","\n","\n","b = b.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n","c = c.remove_columns([\"id\", \"num_samples\", \"path\", \"raw_transcription\", \"gender\", \"lang_id\", \"language\", \"lang_group_id\"])\n","\n","a = a.rename_column(\"text\", \"sentence\")\n","c = c.rename_column(\"transcription\", \"sentence\")\n","\n","\n","print(a)\n","print(b)\n","print(c)\n","\n","from datasets import Audio\n","\n","a = a.cast_column(\"audio\", Audio(sampling_rate=16000))\n","b = b.cast_column(\"audio\", Audio(sampling_rate=16000))\n","c = c.cast_column(\"audio\", Audio(sampling_rate=16000))\n","\n","from datasets import concatenate_datasets\n","\n","combined_train = concatenate_datasets([a['train'], b['train'], c['train']])\n","combined_test = concatenate_datasets([a['test'], b['test'], c['test']])\n","combined_test = combined_test.shuffle(seed=42)\n","combined_valid = concatenate_datasets([a['validation'], b['validation'], c['validation']])\n","\n","combined_dataset = IterableDatasetDict({\n","    'train': combined_train,\n","    \"validation\": combined_valid,\n","    'test': combined_test\n","})\n","\n","dataset = combined_dataset\n","print(dataset)"]},{"cell_type":"markdown","metadata":{},"source":["## Small Whisper"]},{"cell_type":"markdown","metadata":{},"source":["### Base Model"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-08-25T19:52:20.872225Z","iopub.status.busy":"2025-08-25T19:52:20.871952Z","iopub.status.idle":"2025-08-25T20:14:20.809056Z","shell.execute_reply":"2025-08-25T20:14:20.808333Z","shell.execute_reply.started":"2025-08-25T19:52:20.872206Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1334/1334 [21:55<00:00,  1.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Test WER: 43.62%\n","Test CER: 21.61%\n","Test Normalized WER: 36.69%\n"]}],"source":["import re\n","import string\n","from transformers import WhisperForConditionalGeneration, WhisperProcessor\n","import torch\n","import evaluate\n","from tqdm import tqdm\n","from peft import PeftModel\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","\n","whisper_model = WhisperForConditionalGeneration.from_pretrained(\n","    \"openai/whisper-small\",\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-Small\")\n","whisper_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"el\", task=\"transcribe\")\n","\n","wer_metric = evaluate.load(\"wer\")\n","cer_metric = evaluate.load(\"cer\")\n","\n","# Normalize text for WER computation: lowercase, remove punctuation, standardize whitespace.\n","def normalize_text(text):\n","    text = text.lower()\n","    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","\n","def compute_wer(references, hypotheses):\n","    if len(references) != len(hypotheses):\n","        raise ValueError(\"References and hypotheses must have the same length\")\n","    wer_score = wer_metric.compute(predictions=hypotheses, references=references)\n","    return 100 * wer_score\n","\n","def compute_normalized_wer(references, hypotheses):\n","    \"\"\"Compute WER after normalizing references and hypotheses.\"\"\"\n","    normalized_refs = [normalize_text(ref) for ref in references]\n","    normalized_hyps = [normalize_text(hyp) for hyp in hypotheses]\n","    wer_score = wer_metric.compute(predictions=normalized_hyps, references=normalized_refs)\n","    return 100 * wer_score\n","\n","pred_strs = []\n","ref_strs = []\n","\n","for item in tqdm(dataset[\"test\"]):\n","    audio_array = item[\"audio\"][\"array\"]\n","    sampling_rate = item[\"audio\"][\"sampling_rate\"]\n","    reference = item[\"sentence\"]\n","\n","    # Convert audio to input features\n","    input_features = processor(\n","        audio_array,\n","        sampling_rate=sampling_rate,\n","        return_tensors=\"pt\"\n","    ).input_features.to(device, dtype=whisper_model.dtype)\n","\n","    # Decode using the fine-tuned whisper model\n","    with torch.no_grad():\n","        pred_ids = whisper_model.generate(input_features, max_length=225)\n","\n","    prediction = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n","\n","    pred_strs.append(prediction)\n","    ref_strs.append(reference)\n","\n","wer_score = compute_wer(ref_strs, pred_strs)\n","print(f\"Test WER: {wer_score:.2f}%\")\n","\n","cer_score = 100 * cer_metric.compute(predictions=pred_strs, references=ref_strs)\n","print(f\"Test CER: {cer_score:.2f}%\")\n","\n","normalized_wer_score = compute_normalized_wer(ref_strs, pred_strs)\n","print(f\"Test Normalized WER: {normalized_wer_score:.2f}%\")\n","\n","import torch\n","import gc\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Fine-tuned Model"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-08-25T20:14:20.810458Z","iopub.status.busy":"2025-08-25T20:14:20.810253Z","iopub.status.idle":"2025-08-25T20:36:41.514061Z","shell.execute_reply":"2025-08-25T20:36:41.513375Z","shell.execute_reply.started":"2025-08-25T20:14:20.810442Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'target_parameters', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n","  warnings.warn(\n","100%|██████████| 1334/1334 [22:16<00:00,  1.00s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Test WER fine med: 30.31%\n","Test CER fine med: 13.28%\n","Test Normalized WER fine med: 26.54%\n"]}],"source":["\n","base_whisper = WhisperForConditionalGeneration.from_pretrained(\n","    \"openai/whisper-small\",\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","\n","# Load LoRA weights\n","ft_whisper = PeftModel.from_pretrained(\n","    base_whisper, \n","    \"Vardis/Whisper-Small-Greek\"\n",")\n","\n","# Merge LoRA → base weights\n","whisper_model = ft_whisper.merge_and_unload().to(device)\n","\n","processor = WhisperProcessor.from_pretrained(\"Vardis/Whisper-Small-Greek\")\n","whisper_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"el\", task=\"transcribe\")\n","\n","wer_metric = evaluate.load(\"wer\")\n","cer_metric = evaluate.load(\"cer\")\n","\n","def normalize_text(text):\n","    text = text.lower()\n","    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","\n","def compute_wer(references, hypotheses):\n","    if len(references) != len(hypotheses):\n","        raise ValueError(\"References and hypotheses must have the same length\")\n","    wer_score = wer_metric.compute(predictions=hypotheses, references=references)\n","    return 100 * wer_score\n","\n","def compute_normalized_wer(references, hypotheses):\n","    \"\"\"Compute WER after normalizing references and hypotheses.\"\"\"\n","    normalized_refs = [normalize_text(ref) for ref in references]\n","    normalized_hyps = [normalize_text(hyp) for hyp in hypotheses]\n","    wer_score = wer_metric.compute(predictions=normalized_hyps, references=normalized_refs)\n","    return 100 * wer_score\n","\n","pred_strs = []\n","ref_strs = []\n","\n","for item in tqdm(dataset[\"test\"]):\n","    audio_array = item[\"audio\"][\"array\"]\n","    sampling_rate = item[\"audio\"][\"sampling_rate\"]\n","    reference = item[\"sentence\"]\n","\n","    input_features = processor(\n","        audio_array,\n","        sampling_rate=sampling_rate,\n","        return_tensors=\"pt\"\n","    ).input_features.to(device, dtype=whisper_model.dtype)\n","\n","    with torch.no_grad():\n","        pred_ids = whisper_model.generate(input_features, max_length=225)\n","\n","    prediction = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n","\n","    pred_strs.append(prediction)\n","    ref_strs.append(reference)\n","\n","wer_score = compute_wer(ref_strs, pred_strs)\n","print(f\"Test WER fine med: {wer_score:.2f}%\")\n","\n","cer_score = 100 * cer_metric.compute(predictions=pred_strs, references=ref_strs)\n","print(f\"Test CER fine med: {cer_score:.2f}%\")\n","\n","normalized_wer_score = compute_normalized_wer(ref_strs, pred_strs)\n","print(f\"Test Normalized WER fine med: {normalized_wer_score:.2f}%\")\n","\n","import torch\n","import gc\n","\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["## Medium Whisper"]},{"cell_type":"markdown","metadata":{},"source":["### Base Model"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-08-25T20:36:41.515111Z","iopub.status.busy":"2025-08-25T20:36:41.514884Z","iopub.status.idle":"2025-08-25T21:16:56.527163Z","shell.execute_reply":"2025-08-25T21:16:56.526479Z","shell.execute_reply.started":"2025-08-25T20:36:41.515082Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd64d8fe4afc4c509d2e9fddac90b190","version_major":2,"version_minor":0},"text/plain":["config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84136870fe994b669c3d7aaa33707de2","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38f5fdf3cfbe471582b9a41c95d12db2","version_major":2,"version_minor":0},"text/plain":["generation_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d38d9a7a4da24b189fccfa5eff339553","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d876ed771fb146e6b69ea5d88716984c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1462e3df24a4fed9c2a39ddfa0d344d","version_major":2,"version_minor":0},"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3addcf77bd524f9c96af1d8ab0c7477d","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"918a6872853d49349c9426341a8f774b","version_major":2,"version_minor":0},"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd6ab0c2cb44451ba097463c66e273a9","version_major":2,"version_minor":0},"text/plain":["normalizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5bf9f05b5c3482d971418805204c2ea","version_major":2,"version_minor":0},"text/plain":["added_tokens.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"504b902c3a6645169d78b2dc1700970b","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1334/1334 [39:54<00:00,  1.80s/it] \n"]},{"name":"stdout","output_type":"stream","text":["Test WER: 34.71%\n","Test CER: 19.30%\n","Test Normalized WER: 27.21%\n"]}],"source":["whisper_model = WhisperForConditionalGeneration.from_pretrained(\n","    \"openai/whisper-medium\",\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\")\n","whisper_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"el\", task=\"transcribe\")\n","\n","wer_metric = evaluate.load(\"wer\")\n","cer_metric = evaluate.load(\"cer\")\n","\n","def normalize_text(text):\n","    text = text.lower()\n","    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","\n","def compute_wer(references, hypotheses):\n","    if len(references) != len(hypotheses):\n","        raise ValueError(\"References and hypotheses must have the same length\")\n","    wer_score = wer_metric.compute(predictions=hypotheses, references=references)\n","    return 100 * wer_score\n","\n","def compute_normalized_wer(references, hypotheses):\n","    \"\"\"Compute WER after normalizing references and hypotheses.\"\"\"\n","    normalized_refs = [normalize_text(ref) for ref in references]\n","    normalized_hyps = [normalize_text(hyp) for hyp in hypotheses]\n","    wer_score = wer_metric.compute(predictions=normalized_hyps, references=normalized_refs)\n","    return 100 * wer_score\n","\n","pred_strs = []\n","ref_strs = []\n","\n","for item in tqdm(dataset[\"test\"]):\n","    audio_array = item[\"audio\"][\"array\"]\n","    sampling_rate = item[\"audio\"][\"sampling_rate\"]\n","    reference = item[\"sentence\"]\n","\n","    input_features = processor(\n","        audio_array,\n","        sampling_rate=sampling_rate,\n","        return_tensors=\"pt\"\n","    ).input_features.to(device, dtype=whisper_model.dtype)\n","\n","    with torch.no_grad():\n","        pred_ids = whisper_model.generate(input_features, max_length=225)\n","\n","    prediction = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n","\n","    pred_strs.append(prediction)\n","    ref_strs.append(reference)\n","\n","wer_score = compute_wer(ref_strs, pred_strs)\n","print(f\"Test WER: {wer_score:.2f}%\")\n","\n","cer_score = 100 * cer_metric.compute(predictions=pred_strs, references=ref_strs)\n","print(f\"Test CER: {cer_score:.2f}%\")\n","\n","normalized_wer_score = compute_normalized_wer(ref_strs, pred_strs)\n","print(f\"Test Normalized WER: {normalized_wer_score:.2f}%\")\n","\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["### Fine-tuned Model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-08-25T21:16:56.528852Z","iopub.status.busy":"2025-08-25T21:16:56.528602Z","iopub.status.idle":"2025-08-25T21:59:58.181648Z","shell.execute_reply":"2025-08-25T21:59:58.180878Z","shell.execute_reply.started":"2025-08-25T21:16:56.528835Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"516309c69ef6482abda1d510fb36d2b3","version_major":2,"version_minor":0},"text/plain":["config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd2e2a975d3147fabdae01352240d6b5","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"835a9eb7126f4986934ed958ad28bf79","version_major":2,"version_minor":0},"text/plain":["generation_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f36d976ab4e1465f9d818052b3fe0593","version_major":2,"version_minor":0},"text/plain":["adapter_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ecf1e1ca162457386c102c2d44e6957","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/75.6M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"023a6d0f97ed45739d7853adc55ff049","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/356 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c5a8ce4cd1142ffb44b9315114ae3c2","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88cb84ac3bc64f8cbbfc6f13f32bbd04","version_major":2,"version_minor":0},"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d245ad5fe3f649b0a3ce81a8e29ef4f4","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"816690fe260349e2bb0a515d6602e62d","version_major":2,"version_minor":0},"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"462bf1ed3a9a4ee49ccda387acb75ad6","version_major":2,"version_minor":0},"text/plain":["normalizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9c0ac4ed15e462bba23674ba4a5a7c8","version_major":2,"version_minor":0},"text/plain":["added_tokens.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"527ac83e65834c60a290aff704ad8495","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1334/1334 [42:47<00:00,  1.92s/it] \n"]},{"name":"stdout","output_type":"stream","text":["Test WER fine med: 19.45%\n","Test CER fine med: 8.96%\n","Test Normalized WER fine med: 16.17%\n"]}],"source":["base_whisper = WhisperForConditionalGeneration.from_pretrained(\n","    \"openai/whisper-Medium\",\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","\n","ft_whisper = PeftModel.from_pretrained(\n","    base_whisper, \n","    \"Vardis/Whisper-Medium-Greek\"\n",")\n","\n","whisper_model = ft_whisper.merge_and_unload().to(device)\n","\n","processor = WhisperProcessor.from_pretrained(\"Vardis/Whisper-Medium-Greek\")\n","whisper_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"el\", task=\"transcribe\")\n","\n","wer_metric = evaluate.load(\"wer\")\n","cer_metric = evaluate.load(\"cer\")\n","\n","def normalize_text(text):\n","    text = text.lower()\n","    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","\n","def compute_wer(references, hypotheses):\n","    if len(references) != len(hypotheses):\n","        raise ValueError(\"References and hypotheses must have the same length\")\n","    wer_score = wer_metric.compute(predictions=hypotheses, references=references)\n","    return 100 * wer_score\n","\n","def compute_normalized_wer(references, hypotheses):\n","    \"\"\"Compute WER after normalizing references and hypotheses.\"\"\"\n","    normalized_refs = [normalize_text(ref) for ref in references]\n","    normalized_hyps = [normalize_text(hyp) for hyp in hypotheses]\n","    wer_score = wer_metric.compute(predictions=normalized_hyps, references=normalized_refs)\n","    return 100 * wer_score\n","\n","pred_strs = []\n","ref_strs = []\n","\n","for item in tqdm(dataset[\"test\"]):\n","    audio_array = item[\"audio\"][\"array\"]\n","    sampling_rate = item[\"audio\"][\"sampling_rate\"]\n","    reference = item[\"sentence\"]\n","\n","    input_features = processor(\n","        audio_array,\n","        sampling_rate=sampling_rate,\n","        return_tensors=\"pt\"\n","    ).input_features.to(device, dtype=whisper_model.dtype)\n","\n","    with torch.no_grad():\n","        pred_ids = whisper_model.generate(input_features, max_length=225)\n","\n","    prediction = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n","\n","    pred_strs.append(prediction)\n","    ref_strs.append(reference)\n","\n","wer_score = compute_wer(ref_strs, pred_strs)\n","print(f\"Test WER fine med: {wer_score:.2f}%\")\n","\n","cer_score = 100 * cer_metric.compute(predictions=pred_strs, references=ref_strs)\n","print(f\"Test CER fine med: {cer_score:.2f}%\")\n","\n","normalized_wer_score = compute_normalized_wer(ref_strs, pred_strs)\n","print(f\"Test Normalized WER fine med: {normalized_wer_score:.2f}%\")\n","\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["## Large Whisper V2"]},{"cell_type":"markdown","metadata":{},"source":["### Base Model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-08-25T21:59:58.183091Z","iopub.status.busy":"2025-08-25T21:59:58.182860Z","iopub.status.idle":"2025-08-25T22:56:35.879362Z","shell.execute_reply":"2025-08-25T22:56:35.878691Z","shell.execute_reply.started":"2025-08-25T21:59:58.183074Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f91c38b4865d4c7cb8b51f1f47733216","version_major":2,"version_minor":0},"text/plain":["config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec99cc7f98c7440fbff6cecfa6a32fc9","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10e92c4739a24a34b2eb4518f75182b8","version_major":2,"version_minor":0},"text/plain":["generation_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8d510ac31924a9fbc93054452e83761","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db7ed90ce655472e867228dd98a64a15","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e7fad46de0bf4ba39e6555974c2929ca","version_major":2,"version_minor":0},"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"495645c97af24ff7b331f65040cac9c5","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce5ad3e86651449c8560b047977ef483","version_major":2,"version_minor":0},"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ff424a5656444ffbfa70e5ce5d0fbec","version_major":2,"version_minor":0},"text/plain":["normalizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a7c4ae86e2004f40a1fa663a17aa9524","version_major":2,"version_minor":0},"text/plain":["added_tokens.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd10fb1b9947407087459ef67d71bc03","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1334/1334 [55:51<00:00,  2.51s/it] \n"]},{"name":"stdout","output_type":"stream","text":["Test WER: 26.41%\n","Test CER: 14.55%\n","Test Normalized WER: 18.86%\n"]}],"source":["whisper_model = WhisperForConditionalGeneration.from_pretrained(\n","    \"openai/whisper-large-v2\",\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n","whisper_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"el\", task=\"transcribe\")\n","\n","wer_metric = evaluate.load(\"wer\")\n","cer_metric = evaluate.load(\"cer\")\n","\n","def normalize_text(text):\n","    text = text.lower()\n","    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","\n","def compute_wer(references, hypotheses):\n","    if len(references) != len(hypotheses):\n","        raise ValueError(\"References and hypotheses must have the same length\")\n","    wer_score = wer_metric.compute(predictions=hypotheses, references=references)\n","    return 100 * wer_score\n","\n","def compute_normalized_wer(references, hypotheses):\n","    \"\"\"Compute WER after normalizing references and hypotheses.\"\"\"\n","    normalized_refs = [normalize_text(ref) for ref in references]\n","    normalized_hyps = [normalize_text(hyp) for hyp in hypotheses]\n","    wer_score = wer_metric.compute(predictions=normalized_hyps, references=normalized_refs)\n","    return 100 * wer_score\n","\n","pred_strs = []\n","ref_strs = []\n","\n","for item in tqdm(dataset[\"test\"]):\n","    audio_array = item[\"audio\"][\"array\"]\n","    sampling_rate = item[\"audio\"][\"sampling_rate\"]\n","    reference = item[\"sentence\"]\n","\n","    input_features = processor(\n","        audio_array,\n","        sampling_rate=sampling_rate,\n","        return_tensors=\"pt\"\n","    ).input_features.to(device, dtype=whisper_model.dtype)\n","\n","    with torch.no_grad():\n","        pred_ids = whisper_model.generate(input_features, max_length=225)\n","\n","    prediction = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n","\n","    pred_strs.append(prediction)\n","    ref_strs.append(reference)\n","\n","wer_score = compute_wer(ref_strs, pred_strs)\n","print(f\"Test WER: {wer_score:.2f}%\")\n","\n","cer_score = 100 * cer_metric.compute(predictions=pred_strs, references=ref_strs)\n","print(f\"Test CER: {cer_score:.2f}%\")\n","\n","normalized_wer_score = compute_normalized_wer(ref_strs, pred_strs)\n","print(f\"Test Normalized WER: {normalized_wer_score:.2f}%\")\n","\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["### Fine-tuned Model"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-08-26T13:23:55.641398Z","iopub.status.busy":"2025-08-26T13:23:55.641158Z","iopub.status.idle":"2025-08-26T14:22:00.600174Z","shell.execute_reply":"2025-08-26T14:22:00.599041Z","shell.execute_reply.started":"2025-08-26T13:23:55.641382Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4063b31a4be44f3f8519b4f56ca3e7c5","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/356 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60434a5ed934482283889a3f50036dd9","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71218acfdb25462eb76e73d6fb35f6f8","version_major":2,"version_minor":0},"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1ead3c157d44ecfba83c392e8fde20d","version_major":2,"version_minor":0},"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4dc0ca57d6449d7ab3feaa092760c50","version_major":2,"version_minor":0},"text/plain":["normalizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c8618983ee84613806642c2d815dbd5","version_major":2,"version_minor":0},"text/plain":["added_tokens.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c55230f0d5d14a60bd5d5a958e4ade4a","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"744228c7de7e4760b73b0e860b3dde3a","version_major":2,"version_minor":0},"text/plain":["Downloading builder script: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"294e22dfd0e24627a5ee63d28a5d95d4","version_major":2,"version_minor":0},"text/plain":["Downloading builder script: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1334 [00:00<?, ?it/s]Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","100%|██████████| 1334/1334 [57:54<00:00,  2.60s/it] \n"]},{"name":"stdout","output_type":"stream","text":["Test WER fine med: 14.90%\n","Test CER fine med: 8.45%\n","Test Normalized WER fine med: 12.06%\n"]},{"ename":"NameError","evalue":"name 'gc' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3826908341.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Normalized WER fine med: {normalized_wer_score:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"]}],"source":["base_whisper = WhisperForConditionalGeneration.from_pretrained(\n","    \"openai/whisper-large-v2\",\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","\n","ft_whisper = PeftModel.from_pretrained(\n","    base_whisper, \n","    \"Vardis/Whisper-Large-v2-Greek\"\n",")\n","\n","whisper_model = ft_whisper.merge_and_unload().to(device)\n","\n","processor = WhisperProcessor.from_pretrained(\"Vardis/Whisper-Large-v2-Greek\")\n","whisper_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"el\", task=\"transcribe\")\n","\n","wer_metric = evaluate.load(\"wer\")\n","cer_metric = evaluate.load(\"cer\")\n","\n","def normalize_text(text):\n","    text = text.lower()\n","    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","\n","def compute_wer(references, hypotheses):\n","    if len(references) != len(hypotheses):\n","        raise ValueError(\"References and hypotheses must have the same length\")\n","    wer_score = wer_metric.compute(predictions=hypotheses, references=references)\n","    return 100 * wer_score\n","\n","def compute_normalized_wer(references, hypotheses):\n","    \"\"\"Compute WER after normalizing references and hypotheses.\"\"\"\n","    normalized_refs = [normalize_text(ref) for ref in references]\n","    normalized_hyps = [normalize_text(hyp) for hyp in hypotheses]\n","    wer_score = wer_metric.compute(predictions=normalized_hyps, references=normalized_refs)\n","    return 100 * wer_score\n","\n","pred_strs = []\n","ref_strs = []\n","\n","for item in tqdm(dataset[\"test\"]):\n","    audio_array = item[\"audio\"][\"array\"]\n","    sampling_rate = item[\"audio\"][\"sampling_rate\"]\n","    reference = item[\"sentence\"]\n","\n","    input_features = processor(\n","        audio_array,\n","        sampling_rate=sampling_rate,\n","        return_tensors=\"pt\"\n","    ).input_features.to(device, dtype=whisper_model.dtype)\n","\n","    with torch.no_grad():\n","        pred_ids = whisper_model.generate(input_features, max_length=225)\n","\n","    prediction = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n","\n","    pred_strs.append(prediction)\n","    ref_strs.append(reference)\n","\n","wer_score = compute_wer(ref_strs, pred_strs)\n","print(f\"Test WER fine med: {wer_score:.2f}%\")\n","\n","cer_score = 100 * cer_metric.compute(predictions=pred_strs, references=ref_strs)\n","print(f\"Test CER fine med: {cer_score:.2f}%\")\n","\n","normalized_wer_score = compute_normalized_wer(ref_strs, pred_strs)\n","print(f\"Test Normalized WER fine med: {normalized_wer_score:.2f}%\")\n","\n","gc.collect()\n","torch.cuda.empty_cache()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":4}
