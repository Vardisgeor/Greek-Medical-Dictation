{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Greek GPT-2 Fine-tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install datasets==3.6.0\n","!pip install git+https://github.com/huggingface/transformers\n","!pip install librosa\n","!pip install evaluate>=0.30\n","!pip install jiwer\n","!pip install gradio\n","!pip install -q bitsandbytes datasets accelerate loralib\n","!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git@main\n","!pip install evaluate jiwer"]},{"cell_type":"markdown","metadata":{},"source":["## Load Speech Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import load_dataset, IterableDatasetDict\n","import os\n","from datasets import Audio\n","from datasets import concatenate_datasets\n","\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","language = \"Greek\"\n","language_abbr = \"el\"\n","language_abbr2 = \"el_gr\"\n","task = \"transcribe\"\n","\n","\n","a = IterableDatasetDict()\n","b = IterableDatasetDict()\n","c = IterableDatasetDict()\n","\n","\n","a_full = load_dataset(\"Vardis/Greek_Mosel\", split=\"train\")\n","a_temp = a_full.train_test_split(test_size=0.2, seed=42)  # 80% train \n","a_val_test = a_temp[\"test\"].train_test_split(test_size=0.5, seed=42)  # 10% val + 10% test\n","a[\"train\"] = a_temp[\"train\"]\n","a[\"validation\"] = a_val_test[\"train\"]\n","a[\"test\"] = a_val_test[\"test\"]\n","\n","b_full = load_dataset(\"mozilla-foundation/common_voice_11_0\", language_abbr, split=\"train+validation+test\")\n","b_temp = b_full.train_test_split(test_size=0.2, seed=42)\n","b_val_test = b_temp[\"test\"].train_test_split(test_size=0.5, seed=42)\n","b[\"train\"] = b_temp[\"train\"]\n","b[\"validation\"] = b_val_test[\"train\"]\n","b[\"test\"] = b_val_test[\"test\"]\n","\n","c_full = load_dataset(\"google/fleurs\", language_abbr2, split=\"train+validation+test\")\n","c_temp = c_full.train_test_split(test_size=0.2, seed=42)\n","c_val_test = c_temp[\"test\"].train_test_split(test_size=0.5, seed=42)\n","c[\"train\"] = c_temp[\"train\"]\n","c[\"validation\"] = c_val_test[\"train\"]\n","c[\"test\"] = c_val_test[\"test\"]"]},{"cell_type":"code","execution_count":19,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-08-16T00:11:11.799067Z","iopub.status.busy":"2025-08-16T00:11:11.798805Z","iopub.status.idle":"2025-08-16T00:11:13.619803Z","shell.execute_reply":"2025-08-16T00:11:13.619013Z","shell.execute_reply.started":"2025-08-16T00:11:11.799047Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["IterableDatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 3100\n","    })\n","    validation: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 388\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 388\n","    })\n","})\n","IterableDatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 4248\n","    })\n","    validation: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 531\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 532\n","    })\n","})\n","IterableDatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 3308\n","    })\n","    validation: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 414\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 414\n","    })\n","})\n","IterableDatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 10656\n","    })\n","    validation: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 1333\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 1334\n","    })\n","})\n"]}],"source":["b = b.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n","c = c.remove_columns([\"id\", \"num_samples\", \"path\", \"raw_transcription\", \"gender\", \"lang_id\", \"language\", \"lang_group_id\"])\n","\n","a = a.rename_column(\"text\", \"sentence\")\n","c = c.rename_column(\"transcription\", \"sentence\")\n","\n","print(a)\n","print(b)\n","print(c)\n","\n","a = a.cast_column(\"audio\", Audio(sampling_rate=16000))\n","b = b.cast_column(\"audio\", Audio(sampling_rate=16000))\n","c = c.cast_column(\"audio\", Audio(sampling_rate=16000))\n","\n","combined_train = concatenate_datasets([a['train'], b['train'], c['train']])\n","combined_test = concatenate_datasets([a['test'], b['test'], c['test']])\n","combined_valid = concatenate_datasets([a['validation'], b['validation'], c['validation']])\n","\n","combined_dataset = IterableDatasetDict({\n","    'train': combined_train,\n","    \"validation\": combined_valid,\n","    'test': combined_test\n","})\n","\n","dataset = combined_dataset\n","print(dataset)"]},{"cell_type":"markdown","metadata":{},"source":["## Medical Dataset"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-08-16T00:10:32.244177Z","iopub.status.busy":"2025-08-16T00:10:32.243557Z","iopub.status.idle":"2025-08-16T00:10:32.730384Z","shell.execute_reply":"2025-08-16T00:10:32.729759Z","shell.execute_reply.started":"2025-08-16T00:10:32.244154Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['text'],\n","        num_rows: 16548\n","    })\n","    validation: Dataset({\n","        features: ['text'],\n","        num_rows: 1839\n","    })\n","    test: Dataset({\n","        features: ['text'],\n","        num_rows: 2043\n","    })\n","})\n"]}],"source":["from datasets import load_dataset, DatasetDict\n","\n","dataset = load_dataset(\"Vardis/Greek_Medical_Text\")\n","\n","# split into train+validation and test\n","split_dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n","\n","# split the train set into train and validation\n","train_valid_split = split_dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n","\n","medical_dataset = DatasetDict({\n","    \"train\": train_valid_split[\"train\"],\n","    \"validation\": train_valid_split[\"test\"],\n","    \"test\": split_dataset[\"test\"]\n","})\n","\n","print(medical_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["## Merge and Shuffle Text Datasets\n","\n","This step combines all sentences/texts from ds1 and ds2, shuffles the combined dataset, and splits it into final training, validation, and test sets.\n"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2025-08-16T00:26:25.841736Z","iopub.status.busy":"2025-08-16T00:26:25.840997Z","iopub.status.idle":"2025-08-16T00:26:25.846613Z","shell.execute_reply":"2025-08-16T00:26:25.845920Z","shell.execute_reply.started":"2025-08-16T00:26:25.841709Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text'],\n","        num_rows: 31727\n","    })\n","    validation: Dataset({\n","        features: ['text'],\n","        num_rows: 607\n","    })\n","    test: Dataset({\n","        features: ['text'],\n","        num_rows: 1419\n","    })\n","})"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import Dataset, DatasetDict, concatenate_datasets\n","\n","all_sentences = []\n","for split in [\"train\", \"validation\", \"test\"]:\n","    part = dataset[split].to_dataset() if hasattr(dataset[split], \"to_dataset\") else dataset[split]\n","    all_sentences.extend(part[\"sentence\"])\n","\n","all_texts = []\n","for split in [\"train\", \"validation\", \"test\"]:\n","    all_texts.extend(medical_dataset[split][\"text\"])\n","\n","dataset_all = Dataset.from_dict({\"sentence\": all_sentences})\n","medical_dataset_all = Dataset.from_dict({\"text\": all_texts})\n","\n","final_all = concatenate_datasets([dataset_all, medical_dataset_all]).shuffle(seed=42)\n","\n","train_test = final_all.train_test_split(test_size=0.06, seed=42)\n","val_test = train_test[\"test\"].train_test_split(test_size=0.7, seed=42)\n","\n","final_ds = DatasetDict({\n","    \"train\": train_test[\"train\"],\n","    \"validation\": val_test[\"train\"],\n","    \"test\": val_test[\"test\"]\n","})\n","\n","print(final_ds)"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenize Dataset \n","\n","This step loads the Greek GPT-2 tokenizer, sets the padding token, and tokenizes all texts in final_ds with truncation and padding. Labels are created as a copy of the input IDs for language modeling.\n"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2025-08-16T00:26:27.836555Z","iopub.status.busy":"2025-08-16T00:26:27.835995Z","iopub.status.idle":"2025-08-16T00:26:37.578879Z","shell.execute_reply":"2025-08-16T00:26:37.578063Z","shell.execute_reply.started":"2025-08-16T00:26:27.836520Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40ea50bd5f034c4d9bd5851b451472e1","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/31727 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15710d3dfa054a1da4145e87722abb1e","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/607 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f14862009dbc48feba551f0bd8209d1a","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1419 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, EvalPrediction\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"lighteternal/gpt2-finetuned-greek\")\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","def tokenize_function(examples):\n","    tokens = tokenizer(\n","        examples[\"text\"],\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=256\n","    )\n","    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n","    return tokens\n","\n","tokenized_dataset = final_ds.map(tokenize_function, batched=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Apply LoRA to GPT-2 \n","\n","The Greek GPT-2 model is loaded and moved to the GPU if available. LoRA is applied to the attention and projection layers to enable parameter-efficient fine-tuning.\n"]},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2025-08-16T00:36:35.427352Z","iopub.status.busy":"2025-08-16T00:36:35.427023Z","iopub.status.idle":"2025-08-16T00:36:36.879383Z","shell.execute_reply":"2025-08-16T00:36:36.878592Z","shell.execute_reply.started":"2025-08-16T00:36:35.427327Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 1,622,016 || all params: 126,061,824 || trainable%: 1.2867\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n","  warnings.warn(\n"]}],"source":["import torch\n","from transformers import AutoModelForCausalLM\n","from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel\n","\n","# Load the base GPT-2 model\n","model = AutoModelForCausalLM.from_pretrained(\"lighteternal/gpt2-finetuned-greek\")\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","\n","# Define LoRA Config\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    target_modules=[\"c_attn\", \"c_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","# Wrap the model with LoRA configuration\n","peft_model = get_peft_model(model, lora_config)\n","\n","peft_model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{},"source":["## Compute Perplexity\n","\n","This function calculates the average loss and perplexity on evaluation data."]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2025-08-16T00:36:38.202175Z","iopub.status.busy":"2025-08-16T00:36:38.201233Z","iopub.status.idle":"2025-08-16T00:36:38.211307Z","shell.execute_reply":"2025-08-16T00:36:38.210387Z","shell.execute_reply.started":"2025-08-16T00:36:38.202132Z"},"trusted":true},"outputs":[],"source":["import math\n","from transformers import AutoTokenizer, AutoModelForCausalLM, EvalPrediction\n","\n","\n","def compute_metrics(eval_pred: EvalPrediction):\n","    label_ids = eval_pred.label_ids.astype(int)\n","\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id or tokenizer.eos_token_id\n","\n","    decoded_labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    total_loss = 0.0\n","    total_tokens = 0\n","\n","    for ref_text in decoded_labels:\n","        inputs = tokenizer(ref_text, return_tensors=\"pt\")\n","        if torch.cuda.is_available():\n","            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n","            model.to('cuda')\n","        with torch.no_grad():\n","            outputs = peft_model(**inputs, labels=inputs[\"input_ids\"])\n","        loss = outputs.loss.item()\n","        total_loss += loss * inputs[\"input_ids\"].size(1)\n","        total_tokens += inputs[\"input_ids\"].size(1)\n","\n","    avg_loss = total_loss / total_tokens\n","    perplexity = math.exp(avg_loss)\n","\n","    return {\"perplexity\": perplexity}\n"]},{"cell_type":"markdown","metadata":{},"source":["## Training Setup"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2025-08-16T00:38:10.012294Z","iopub.status.busy":"2025-08-16T00:38:10.011764Z","iopub.status.idle":"2025-08-16T00:38:10.064405Z","shell.execute_reply":"2025-08-16T00:38:10.063787Z","shell.execute_reply.started":"2025-08-16T00:38:10.012270Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using `include_inputs_for_metrics` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Please use `include_for_metrics` list argument instead.\n","No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"]}],"source":["from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./gpt2-Greek-Medical\",\n","    num_train_epochs=30,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    gradient_accumulation_steps=2,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_steps=50,\n","    include_inputs_for_metrics=True,\n","    fp16=True,\n","    learning_rate=5e-5,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"perplexity\",\n","    greater_is_better=False,\n","    report_to=\"none\"\n",")\n","\n","# The data collator will handle batching and padding\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","trainer = Trainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset['train'],\n","    eval_dataset=tokenized_dataset['validation'].select(range(60)), # we used a smaller subset to fit GPU memory\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":93,"metadata":{"execution":{"iopub.execute_input":"2025-08-16T00:38:12.358947Z","iopub.status.busy":"2025-08-16T00:38:12.358640Z","iopub.status.idle":"2025-08-16T07:22:36.746179Z","shell.execute_reply":"2025-08-16T07:22:36.745464Z","shell.execute_reply.started":"2025-08-16T00:38:12.358926Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='29760' max='29760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [29760/29760 6:44:23, Epoch 30/30]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Perplexity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.954600</td>\n","      <td>4.202718</td>\n","      <td>44.991269</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.953000</td>\n","      <td>4.167151</td>\n","      <td>43.984107</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3.927700</td>\n","      <td>4.142826</td>\n","      <td>43.114071</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>3.925500</td>\n","      <td>4.134766</td>\n","      <td>42.543837</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>3.898000</td>\n","      <td>4.117559</td>\n","      <td>42.029292</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>3.885400</td>\n","      <td>4.104939</td>\n","      <td>41.571901</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>3.860700</td>\n","      <td>4.090889</td>\n","      <td>41.216248</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>3.859100</td>\n","      <td>4.071882</td>\n","      <td>40.839166</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>3.820800</td>\n","      <td>4.070119</td>\n","      <td>40.514361</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>3.816400</td>\n","      <td>4.058827</td>\n","      <td>40.221701</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>3.832600</td>\n","      <td>4.045707</td>\n","      <td>39.995421</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>3.837700</td>\n","      <td>4.042167</td>\n","      <td>39.731023</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>3.808000</td>\n","      <td>4.036338</td>\n","      <td>39.587170</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>3.811800</td>\n","      <td>4.034511</td>\n","      <td>39.369746</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>3.830800</td>\n","      <td>4.032938</td>\n","      <td>39.293628</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>3.777300</td>\n","      <td>4.031011</td>\n","      <td>39.072403</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>3.806600</td>\n","      <td>4.025128</td>\n","      <td>38.982950</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>3.797000</td>\n","      <td>4.022670</td>\n","      <td>38.857038</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>3.776000</td>\n","      <td>4.017247</td>\n","      <td>38.744315</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>3.779300</td>\n","      <td>4.015795</td>\n","      <td>38.702027</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>3.787300</td>\n","      <td>4.009095</td>\n","      <td>38.552335</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>3.762300</td>\n","      <td>4.009308</td>\n","      <td>38.488310</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>3.810600</td>\n","      <td>4.003092</td>\n","      <td>38.436610</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>3.764500</td>\n","      <td>4.002601</td>\n","      <td>38.377568</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>3.771700</td>\n","      <td>4.002158</td>\n","      <td>38.333139</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>3.768700</td>\n","      <td>4.001987</td>\n","      <td>38.316474</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>3.731700</td>\n","      <td>3.998666</td>\n","      <td>38.268585</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>3.775100</td>\n","      <td>3.998754</td>\n","      <td>38.246208</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>3.736800</td>\n","      <td>3.999624</td>\n","      <td>38.224767</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>3.789000</td>\n","      <td>3.999377</td>\n","      <td>38.227331</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=29760, training_loss=3.8237653009353147, metrics={'train_runtime': 24263.8716, 'train_samples_per_second': 39.227, 'train_steps_per_second': 1.227, 'total_flos': 1.2672153970016256e+17, 'train_loss': 3.8237653009353147, 'epoch': 30.0})"]},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["## Push Model and Tokenizer to Hugging Face Hub\n"]},{"cell_type":"code","execution_count":94,"metadata":{"execution":{"iopub.execute_input":"2025-08-16T07:22:36.747684Z","iopub.status.busy":"2025-08-16T07:22:36.747413Z","iopub.status.idle":"2025-08-16T07:22:43.603881Z","shell.execute_reply":"2025-08-16T07:22:43.603221Z","shell.execute_reply.started":"2025-08-16T07:22:36.747665Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cbfe8feb9bb946cb9aa255a16469daa6","version_major":2,"version_minor":0},"text/plain":["Uploading...:   0%|          | 0.00/6.50M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8378b860e254a1ea205aa0ff8caf4d8","version_major":2,"version_minor":0},"text/plain":["README.md: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Vardis/Medical_Speech_Greek_GPT2/commit/b419f7da5adbed1b244d05962881c05b003ca251', commit_message='Upload tokenizer', commit_description='', oid='b419f7da5adbed1b244d05962881c05b003ca251', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Vardis/Medical_Speech_Greek_GPT2', endpoint='https://huggingface.co', repo_type='model', repo_id='Vardis/Medical_Speech_Greek_GPT2'), pr_revision=None, pr_num=None)"]},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":["peft_model.push_to_hub(\"Vardis/Medical_Speech_Greek_GPT2\", token=\"################\")\n","tokenizer.push_to_hub(\"Vardis/Medical_Speech_Greek_GPT2\", token=\"################\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":4}
